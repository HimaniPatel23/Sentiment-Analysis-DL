{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.10"
    },
    "colab": {
      "name": "IMDB Sentiment Analysis using LSTM.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0t2Fx6Z_ozir",
        "colab_type": "text"
      },
      "source": [
        "# Sentiment Analysis on IMDB Data using Long-short term memory"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GAL7jhweozis",
        "colab_type": "text"
      },
      "source": [
        "> <i> Sentiment classification is the task of looking at a piece of text and telling if someone likes or dislikes the thing they’re talking about. </i> \n",
        "[[1](https://towardsdatascience.com/machine-learning-word-embedding-sentiment-classification-using-keras-b83c28087456)]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C42gyWRZozit",
        "colab_type": "text"
      },
      "source": [
        "## IMDB Dataset\n",
        "> IMDB released a adataset containing 50K reviews with their sentiment values. This is a dataset for binary sentiment classification containing substantially more data than previous benchmark datasets. [[2](https://www.kaggle.com/lakshmi25npathi/imdb-dataset-of-50k-movie-reviews)]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OStwIJ8Oozit",
        "colab_type": "text"
      },
      "source": [
        "## Architecture\n",
        "For this classification model, we will build a deep neural network for sentiment classification using Word embeddings. \n",
        "The model will consitst 4 differnet modules which are described in the architecture of the model is described below:<br/>\n",
        "![Architecture](architeacture.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6p3hde3Joziu",
        "colab_type": "text"
      },
      "source": [
        "### Modules\n",
        "> <b>Text</b><br/>\n",
        "IMDB reviews dataset includes the cleaned reviews (removed symbols, lower case etc). <br/>\n",
        "<br/>\n",
        "\n",
        "> <b> Embedding Layer</b> <br/>\n",
        "Word Embedding is a representation of text where words that have the same meaning have a similar representation. In other words it represents words in a coordinate system where related words, based on a corpus of relationships, are placed closer together. In the deep learning frameworks such as TensorFlow, Keras, this part is usually handled by an embedding layer which stores a lookup table to map the words represented by numeric indexes to their dense vector representations.<br/>\n",
        "<br/>\n",
        "\n",
        "> <b> Deep Network LSTM </b><br/>\n",
        "Deep network takes the sequence of embedding vectors as input and converts them to a compressed representation. The compressed representation effectively captures all the information in the sequence of words in the text. The deep network part is usually an RNN or some architecture of it like LSTM/GRU. The dropout is added to overcome the tendency to overfit, which is a very common problem with RNN based networks. Please refer [here](https://towardsdatascience.com/illustrated-guide-to-lstms-and-gru-s-a-step-by-step-explanation-44e9eb85bf21) for detailed discussion on LSTM,GRU.\n",
        "<br/>\n",
        "\n",
        "> <b> Fully Connected Layer </b><br/>\n",
        "The fully connected layer takes the deep representation from the RNN/LSTM/GRU and transforms it into the final output classes or class scores. This component is comprised of fully connected layers along with batch normalization and optionally dropout layers for regularization. <br/>\n",
        "<br/>\n",
        "\n",
        "> <b> Output Layer </b><br/>\n",
        "Based on the problem at hand, this layer can have either Sigmoid for binary classification or Softmax for both binary and multi classification output.<br/>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rsQMCxWGoziu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## input Libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import re\n",
        "import string\n",
        "from tqdm import tqdm\n",
        "import math\n",
        "import tensorflow as tf\n",
        "import sklearn"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LfGytnLvo2Hc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%tensorflow_version 2.x"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XoizmkZ5o6XS",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        },
        "outputId": "19c51976-99cf-4fd4-810a-4a70783d3312"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/gdrive')"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6xhIUAa-oziy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Read dataset from data folder\n",
        "dataset = pd.read_csv('/gdrive/My Drive/data/IMDB Dataset.csv')"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SVUe8HxSozi1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 202
        },
        "outputId": "82ee4146-3b92-4a18-a756-e3060c6944f7"
      },
      "source": [
        "dataset.head()"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>review</th>\n",
              "      <th>sentiment</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>One of the other reviewers has mentioned that ...</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>I thought this was a wonderful way to spend ti...</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Basically there's a family where a little boy ...</td>\n",
              "      <td>negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                              review sentiment\n",
              "0  One of the other reviewers has mentioned that ...  positive\n",
              "1  A wonderful little production. <br /><br />The...  positive\n",
              "2  I thought this was a wonderful way to spend ti...  positive\n",
              "3  Basically there's a family where a little boy ...  negative\n",
              "4  Petter Mattei's \"Love in the Time of Money\" is...  positive"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lcBXjh4sozi4",
        "colab_type": "text"
      },
      "source": [
        "## Data Pre-processing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w8LHoE9Yozi5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Label encode sentiment \n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "lb = LabelEncoder()\n",
        "\n",
        "dataset['sentiment'] = lb.fit_transform(dataset['sentiment'])"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YYccMhicozi7",
        "colab_type": "text"
      },
      "source": [
        "### Label Encoding\n",
        "| Sentiment | Label \n",
        "| :- | -: \n",
        "| Positive | 1\n",
        "| Negative | 0"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1YkUAju8ozi8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Function to remove all the html tags from test\n",
        "def cleanhtml(raw_html):\n",
        "    cleanr = re.compile('<.*?>|&([a-z0-9]+|#[0-9]{1,6}|#x[0-9a-f]{1,6});')\n",
        "    cleantext = re.sub(cleanr, '', raw_html)\n",
        "    return cleantext"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Eu6R8s81ozi_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Function to remove all the numbers from text\n",
        "def remove_numbers(text):\n",
        "    text1 = re.sub(r'\\d+', '', text)\n",
        "    return text1"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ekMT9X8WozjD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Function to remove punctuations and symbols from test\n",
        "def remove_symbol(text):\n",
        "    for char in string.punctuation:\n",
        "        text = text.replace(char,'')\n",
        "    return text"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kL8y1VU8ozjF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "18345410-226e-468f-a9b3-f168eb262eb2"
      },
      "source": [
        "# In this processs lets remove all symbols, html tags, numbers and lower case all the words\n",
        "cleaned_corpus = []\n",
        "\n",
        "for i in tqdm(range(len(dataset['review']))):\n",
        "    temp = str(dataset['review'].iloc[i])\n",
        "\n",
        "    # remove html tage\n",
        "    temp = cleanhtml(temp)\n",
        "\n",
        "    # remove numbers\n",
        "    temp = remove_numbers(temp)\n",
        "\n",
        "    # remove punctuations\n",
        "    temp = remove_symbol(temp)\n",
        "    \n",
        "    # remove all the numbers \n",
        "    temp1 = []\n",
        "    for i in temp.split():\n",
        "        if not i.isdigit():\n",
        "            temp1.append(i)\n",
        "\n",
        "    # join the list\n",
        "    temp = \" \".join(temp1)\n",
        "    \n",
        "    # convert all the words into lower case\n",
        "    temp = str(temp).lower()\n",
        "    \n",
        "    # remove nan\n",
        "    temp = [x for x in temp.split() if str(x) != 'nan']\n",
        "    \n",
        "    cleaned_corpus.append(\" \".join(temp))\n"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 50000/50000 [00:10<00:00, 4969.67it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0BtZIJ2iozjH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 283
        },
        "outputId": "d03700cd-f6d7-4936-d117-a334db1c1560"
      },
      "source": [
        "dataset['sentiment'].value_counts().plot(kind='barh')"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7f05f4ab07f0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0b56M10EozjJ",
        "colab_type": "text"
      },
      "source": [
        "### Note\n",
        "> Looks like out dataset is well-balanced (Same # of review samples in both classes)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H7vA7ySFozjK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Cleaning the data to remove nan(float type) from string\n",
        "corpus = []\n",
        "for i in cleaned_corpus:\n",
        "    temp = []\n",
        "    for j in i.split():\n",
        "        if not j==math.nan:\n",
        "            temp.append(j)\n",
        "    corpus.append(\" \".join(temp))\n",
        "dataset['review'] = [str(i) for i in corpus]"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cuu9q3ySozjO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Let's split the data into training and test data\n",
        "# from sklearn.model_selection import train_test_split\n",
        "# X_train, X_test,y_train, y_test = train_test_split(dataset['review'], dataset['sentiment'], test_size=0.25)"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bN2Ri8aGozjU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train = dataset.loc[:24999, 'review'].values\n",
        "X_test = dataset.loc[25000:, 'review'].values\n",
        "y_train = dataset.loc[:24999, 'sentiment'].values\n",
        "y_test = dataset.loc[25000:, 'sentiment'].values"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "INxQMLuSozjW",
        "colab_type": "text"
      },
      "source": [
        "### Word Embeddings\n",
        "> The word embeddings of our dataset can be learned while training a neural network on the classification problem. Before it can be presented to the network, the text data is first encoded so that each word is represented by a unique integer. This data preparation step can be performed using the Tokenizer API provided with Keras. We add padding to make all the vectors of same length (max_length). Below code converts the text to integer indexes, now ready to be used in Keras embedding layer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SqXQEFgEozjX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Function to get the length of longest review\n",
        "def max_length(clean_review):\n",
        "    max1 = []\n",
        "    for i in clean_review:\n",
        "        temp = len(i.split())\n",
        "        max1.append(temp)\n",
        "    return max1"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AahyjAWJozjZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.python.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.python.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "tokenizer = Tokenizer(lower=False)\n",
        "total_reviews = np.concatenate((X_train, X_test))\n",
        "tokenizer.fit_on_texts(total_reviews)\n",
        "\n",
        "# get the max length of review\n",
        "max_length = max(max_length(dataset['review']))\n",
        "\n",
        "# define vocabulary size\n",
        "vocabulary_size = len(tokenizer.word_index) + 1"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zylYZYwCozjc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train_tokens = tokenizer.texts_to_sequences(X_train)\n",
        "X_test_tokens = tokenizer.texts_to_sequences(X_test)"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6TQ0u5qfozjf",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "a0cdc44b-a20a-4cde-ef22-50bf9a2bb1e9"
      },
      "source": [
        "# Checking the length of review before and after converting to sequences\n",
        "len(X_train_tokens[0]) == len(X_train[0].split())"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bBrKyYf_ozjj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Padding the sequences \n",
        "X_train_pad = pad_sequences(X_train_tokens, maxlen=max_length, padding='post')\n",
        "X_test_pad = pad_sequences(X_test_tokens, maxlen=max_length, padding='post')"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H87nzmHHozjl",
        "colab_type": "text"
      },
      "source": [
        "## Build the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Feu1xAZVozjm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# from tensorflow.keras.models import Sequential\n",
        "# from tensorflow.keras.layers import Embedding, LSTM, GRU, Dense\n",
        "# # from keras.layers.embeddings import Embedding\n",
        "\n",
        "# EMBEDDING_DIM=100\n",
        "# print('Build Model..')\n",
        "\n",
        "# model = Sequential()\n",
        "# model.add(Embedding(vocabulary_size, EMBEDDING_DIM, input_length=max_length))\n",
        "# model.add(GRU(units=32))\n",
        "# model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "# # Since this is binary classification model, lets use binary_classification as loss\n",
        "# model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tOp3kxbkozjp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "EMBEDDING_DIM=100\n",
        "\n",
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Embedding(vocabulary_size, EMBEDDING_DIM, input_length=max_length),\n",
        "    tf.keras.layers.GlobalAveragePooling1D(),\n",
        "    tf.keras.layers.Dense(24, activation='relu'),\n",
        "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
        "])\n",
        "model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dhZ3GfDbozjr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "outputId": "f4fb93b9-0d95-4919-be36-dd358bbdf56e"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_1 (Embedding)      (None, 2441, 100)         21572200  \n",
            "_________________________________________________________________\n",
            "global_average_pooling1d_1 ( (None, 100)               0         \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 24)                2424      \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 1)                 25        \n",
            "=================================================================\n",
            "Total params: 21,574,649\n",
            "Trainable params: 21,574,649\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LX70LKFUozjs",
        "colab_type": "text"
      },
      "source": [
        "## Train the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nP5cHr99ozjt",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 919
        },
        "outputId": "79dcb57f-40c9-48c8-bf42-633574299a3f"
      },
      "source": [
        "print('Train..')\n",
        "\n",
        "model.fit(X_train_pad, y_train, batch_size=128, epochs=25, validation_data=(X_test_pad, y_test), verbose=2)"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train..\n",
            "Epoch 1/25\n",
            "196/196 - 53s - loss: 0.6913 - accuracy: 0.5355 - val_loss: 0.6858 - val_accuracy: 0.5372\n",
            "Epoch 2/25\n",
            "196/196 - 53s - loss: 0.6549 - accuracy: 0.6823 - val_loss: 0.5999 - val_accuracy: 0.7490\n",
            "Epoch 3/25\n",
            "196/196 - 52s - loss: 0.5248 - accuracy: 0.7920 - val_loss: 0.4674 - val_accuracy: 0.8119\n",
            "Epoch 4/25\n",
            "196/196 - 52s - loss: 0.4032 - accuracy: 0.8523 - val_loss: 0.3809 - val_accuracy: 0.8594\n",
            "Epoch 5/25\n",
            "196/196 - 52s - loss: 0.3256 - accuracy: 0.8845 - val_loss: 0.3368 - val_accuracy: 0.8724\n",
            "Epoch 6/25\n",
            "196/196 - 52s - loss: 0.2751 - accuracy: 0.9032 - val_loss: 0.3121 - val_accuracy: 0.8783\n",
            "Epoch 7/25\n",
            "196/196 - 52s - loss: 0.2390 - accuracy: 0.9171 - val_loss: 0.3026 - val_accuracy: 0.8794\n",
            "Epoch 8/25\n",
            "196/196 - 52s - loss: 0.2099 - accuracy: 0.9272 - val_loss: 0.2868 - val_accuracy: 0.8880\n",
            "Epoch 9/25\n",
            "196/196 - 52s - loss: 0.1872 - accuracy: 0.9370 - val_loss: 0.3469 - val_accuracy: 0.8488\n",
            "Epoch 10/25\n",
            "196/196 - 52s - loss: 0.1678 - accuracy: 0.9441 - val_loss: 0.2753 - val_accuracy: 0.8944\n",
            "Epoch 11/25\n",
            "196/196 - 52s - loss: 0.1496 - accuracy: 0.9518 - val_loss: 0.2893 - val_accuracy: 0.8844\n",
            "Epoch 12/25\n",
            "196/196 - 52s - loss: 0.1338 - accuracy: 0.9576 - val_loss: 0.2788 - val_accuracy: 0.8932\n",
            "Epoch 13/25\n",
            "196/196 - 52s - loss: 0.1206 - accuracy: 0.9610 - val_loss: 0.2766 - val_accuracy: 0.8942\n",
            "Epoch 14/25\n",
            "196/196 - 53s - loss: 0.1096 - accuracy: 0.9668 - val_loss: 0.2749 - val_accuracy: 0.8963\n",
            "Epoch 15/25\n",
            "196/196 - 53s - loss: 0.0968 - accuracy: 0.9711 - val_loss: 0.2784 - val_accuracy: 0.8954\n",
            "Epoch 16/25\n",
            "196/196 - 53s - loss: 0.0860 - accuracy: 0.9750 - val_loss: 0.2904 - val_accuracy: 0.8916\n",
            "Epoch 17/25\n",
            "196/196 - 53s - loss: 0.0798 - accuracy: 0.9773 - val_loss: 0.2937 - val_accuracy: 0.8914\n",
            "Epoch 18/25\n",
            "196/196 - 53s - loss: 0.0714 - accuracy: 0.9797 - val_loss: 0.3077 - val_accuracy: 0.8886\n",
            "Epoch 19/25\n",
            "196/196 - 53s - loss: 0.0650 - accuracy: 0.9819 - val_loss: 0.3245 - val_accuracy: 0.8846\n",
            "Epoch 20/25\n",
            "196/196 - 53s - loss: 0.0579 - accuracy: 0.9845 - val_loss: 0.3079 - val_accuracy: 0.8948\n",
            "Epoch 21/25\n",
            "196/196 - 52s - loss: 0.0501 - accuracy: 0.9867 - val_loss: 0.3081 - val_accuracy: 0.8942\n",
            "Epoch 22/25\n",
            "196/196 - 52s - loss: 0.0446 - accuracy: 0.9889 - val_loss: 0.3571 - val_accuracy: 0.8802\n",
            "Epoch 23/25\n",
            "196/196 - 51s - loss: 0.0417 - accuracy: 0.9896 - val_loss: 0.3466 - val_accuracy: 0.8870\n",
            "Epoch 24/25\n",
            "196/196 - 51s - loss: 0.0363 - accuracy: 0.9913 - val_loss: 0.3301 - val_accuracy: 0.8934\n",
            "Epoch 25/25\n",
            "196/196 - 52s - loss: 0.0316 - accuracy: 0.9935 - val_loss: 0.3539 - val_accuracy: 0.8876\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f05f348e7b8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R_VyQtZlwDKR",
        "colab_type": "text"
      },
      "source": [
        "### Test the Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yuWT3Op6ozjv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_1 = \"This movie was awesome I really liked it\"\n",
        "test_2 = \"It was really bad movie\"\n",
        "test = [test_1, test_2]\n",
        "\n",
        "test_sample_token = tokenizer.texts_to_sequences(test)\n",
        "test_sample_pad_seq = pad_sequences(test_sample_token, maxlen=max_length)"
      ],
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q-pTSRPvozjx",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "73e6d4bf-785e-4fa9-c789-26211fb38760"
      },
      "source": [
        "# Predict the output\n",
        "model.predict(x=test_sample_pad_seq)"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.92794365],\n",
              "       [0.58483624]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CN7fsgAgyykC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.10"
    },
    "colab": {
      "name": "IMDB Sentiment Analysis using LSTM.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0t2Fx6Z_ozir",
        "colab_type": "text"
      },
      "source": [
        "# Sentiment Analysis on IMDB Data using Long-short term memory"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GAL7jhweozis",
        "colab_type": "text"
      },
      "source": [
        "> <i> Sentiment classification is the task of looking at a piece of text and telling if someone likes or dislikes the thing they’re talking about. </i> \n",
        "[[1](https://towardsdatascience.com/machine-learning-word-embedding-sentiment-classification-using-keras-b83c28087456)]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C42gyWRZozit",
        "colab_type": "text"
      },
      "source": [
        "## IMDB Dataset\n",
        "> IMDB released a adataset containing 50K reviews with their sentiment values. This is a dataset for binary sentiment classification containing substantially more data than previous benchmark datasets. [[2](https://www.kaggle.com/lakshmi25npathi/imdb-dataset-of-50k-movie-reviews)]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OStwIJ8Oozit",
        "colab_type": "text"
      },
      "source": [
        "## Architecture\n",
        "For this classification model, we will build a deep neural network for sentiment classification using Word embeddings. \n",
        "The model will consitst 4 differnet modules which are described in the architecture of the model is described below:<br/>\n",
        "![Architecture](architeacture.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6p3hde3Joziu",
        "colab_type": "text"
      },
      "source": [
        "### Modules\n",
        "> <b>Text</b><br/>\n",
        "IMDB reviews dataset includes the cleaned reviews (removed symbols, lower case etc). <br/>\n",
        "<br/>\n",
        "\n",
        "> <b> Embedding Layer</b> <br/>\n",
        "Word Embedding is a representation of text where words that have the same meaning have a similar representation. In other words it represents words in a coordinate system where related words, based on a corpus of relationships, are placed closer together. In the deep learning frameworks such as TensorFlow, Keras, this part is usually handled by an embedding layer which stores a lookup table to map the words represented by numeric indexes to their dense vector representations.<br/>\n",
        "<br/>\n",
        "\n",
        "> <b> Deep Network LSTM </b><br/>\n",
        "Deep network takes the sequence of embedding vectors as input and converts them to a compressed representation. The compressed representation effectively captures all the information in the sequence of words in the text. The deep network part is usually an RNN or some architecture of it like LSTM/GRU. The dropout is added to overcome the tendency to overfit, which is a very common problem with RNN based networks. Please refer [here](https://towardsdatascience.com/illustrated-guide-to-lstms-and-gru-s-a-step-by-step-explanation-44e9eb85bf21) for detailed discussion on LSTM,GRU.\n",
        "<br/>\n",
        "\n",
        "> <b> Fully Connected Layer </b><br/>\n",
        "The fully connected layer takes the deep representation from the RNN/LSTM/GRU and transforms it into the final output classes or class scores. This component is comprised of fully connected layers along with batch normalization and optionally dropout layers for regularization. <br/>\n",
        "<br/>\n",
        "\n",
        "> <b> Output Layer </b><br/>\n",
        "Based on the problem at hand, this layer can have either Sigmoid for binary classification or Softmax for both binary and multi classification output.<br/>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rsQMCxWGoziu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Input Libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import re\n",
        "import string\n",
        "from tqdm import tqdm\n",
        "import math\n",
        "import tensorflow as tf\n",
        "import sklearn"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LfGytnLvo2Hc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%tensorflow_version 2.x"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XoizmkZ5o6XS",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        },
        "outputId": "a87ba91e-eea9-4e88-ac47-9582e647d1c0"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/gdrive')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6xhIUAa-oziy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Read dataset from data folder\n",
        "dataset = pd.read_csv('/gdrive/My Drive/data/IMDB Dataset.csv')"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SVUe8HxSozi1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 202
        },
        "outputId": "4e847fa4-2bdf-44ef-873f-2465fe9d7cce"
      },
      "source": [
        "dataset.head()"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>review</th>\n",
              "      <th>sentiment</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>One of the other reviewers has mentioned that ...</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>I thought this was a wonderful way to spend ti...</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Basically there's a family where a little boy ...</td>\n",
              "      <td>negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                              review sentiment\n",
              "0  One of the other reviewers has mentioned that ...  positive\n",
              "1  A wonderful little production. <br /><br />The...  positive\n",
              "2  I thought this was a wonderful way to spend ti...  positive\n",
              "3  Basically there's a family where a little boy ...  negative\n",
              "4  Petter Mattei's \"Love in the Time of Money\" is...  positive"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lcBXjh4sozi4",
        "colab_type": "text"
      },
      "source": [
        "## Data Pre-processing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w8LHoE9Yozi5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Label encode sentiment \n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "lb = LabelEncoder()\n",
        "\n",
        "dataset['sentiment'] = lb.fit_transform(dataset['sentiment'])"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YYccMhicozi7",
        "colab_type": "text"
      },
      "source": [
        "### Label Encoding\n",
        "| Sentiment | Label \n",
        "| :- | -: \n",
        "| Positive | 1\n",
        "| Negative | 0"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1YkUAju8ozi8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Function to remove all the html tags from test\n",
        "def cleanhtml(raw_html):\n",
        "    cleanr = re.compile('<.*?>|&([a-z0-9]+|#[0-9]{1,6}|#x[0-9a-f]{1,6});')\n",
        "    cleantext = re.sub(cleanr, '', raw_html)\n",
        "    return cleantext"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Eu6R8s81ozi_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Function to remove all the numbers from text\n",
        "def remove_numbers(text):\n",
        "    text1 = re.sub(r'\\d+', '', text)\n",
        "    return text1"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ekMT9X8WozjD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Function to remove punctuations and symbols from test\n",
        "def remove_symbol(text):\n",
        "    for char in string.punctuation:\n",
        "        text = text.replace(char,'')\n",
        "    return text"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kL8y1VU8ozjF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# In this processs lets remove all symbols, html tags, numbers and lower case all the words\n",
        "def clean_data(text):\n",
        "  cleaned_corpus = []\n",
        "\n",
        "  for i in tqdm(range(len(text))):\n",
        "      temp = text[i] #str(dataset['review'].iloc[i])\n",
        "\n",
        "      # remove html tage\n",
        "      temp = cleanhtml(temp)\n",
        "\n",
        "      # remove numbers\n",
        "      temp = remove_numbers(temp)\n",
        "\n",
        "      # remove punctuations\n",
        "      temp = remove_symbol(temp)\n",
        "      \n",
        "      # remove all the numbers \n",
        "      temp1 = []\n",
        "      for i in temp.split():\n",
        "          if not i.isdigit():\n",
        "              temp1.append(i)\n",
        "\n",
        "      # join the list\n",
        "      temp = \" \".join(temp1)\n",
        "      \n",
        "      # convert all the words into lower case\n",
        "      temp = str(temp).lower()\n",
        "      \n",
        "      # remove nan\n",
        "      temp = [x for x in temp.split() if str(x) != 'nan']\n",
        "      \n",
        "      cleaned_corpus.append(\" \".join(temp))\n",
        "  return cleaned_corpus"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BSJH6Q4h2ImH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "343e9753-67b2-4f4f-a5c0-e977c39383b6"
      },
      "source": [
        "# Call clean_data method to clean the reviews\n",
        "cleaned_corpus = clean_data(dataset['review'].to_list())"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 50000/50000 [00:10<00:00, 4945.53it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0BtZIJ2iozjH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 283
        },
        "outputId": "b735e9bf-4f9e-4bdb-de0f-90d0dbbdbed6"
      },
      "source": [
        "dataset['sentiment'].value_counts().plot(kind='barh')"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7f11a4c41cc0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0b56M10EozjJ",
        "colab_type": "text"
      },
      "source": [
        "### Note\n",
        "> Looks like out dataset is well-balanced (Same # of review samples in both classes)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H7vA7ySFozjK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Cleaning the data to remove nan(float type) from string\n",
        "corpus = []\n",
        "for i in cleaned_corpus:\n",
        "    temp = []\n",
        "    for j in i.split():\n",
        "        if not j==math.nan:\n",
        "            temp.append(j)\n",
        "    corpus.append(\" \".join(temp))\n",
        "dataset['review'] = [str(i) for i in corpus]"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Awdz84bUKEPy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# dataset_positive = dataset.loc[dataset['sentiment'] == 1]\n",
        "# dataset_negative = dataset.loc[dataset['sentiment'] == 0]"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JOpdVTOnLc0X",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# # Get 50% sample from original data\n",
        "# dataset1 = pd.concat([dataset_positive.sample(frac=0.5, replace=False, random_state=1),dataset_negative.sample(frac=0.5, replace=False, random_state=1)])\n",
        "# dataset1.head()"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gdNwiAYAPilL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# dataset1.shape"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cuu9q3ySozjO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# # Let's split the data into training and test data\n",
        "# from sklearn.model_selection import train_test_split\n",
        "# X_train, X_test,y_train, y_test = train_test_split(dataset1['review'], dataset1['sentiment'], test_size=0.25)"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nqA8VeehP0oq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# # Checking the counts of sentiment in each test and train data\n",
        "# unique, counts = np.unique(y_test, return_counts=True)\n",
        "# print(unique, counts)"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_PY__qBP_jA-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train = dataset.loc[:24999, 'review'].values\n",
        "X_test = dataset.loc[25000:, 'review'].values\n",
        "y_train = dataset.loc[:24999, 'sentiment'].values\n",
        "y_test = dataset.loc[25000:, 'sentiment'].values"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "INxQMLuSozjW",
        "colab_type": "text"
      },
      "source": [
        "### Word Embeddings\n",
        "> The word embeddings of our dataset can be learned while training a neural network on the classification problem. Before it can be presented to the network, the text data is first encoded so that each word is represented by a unique integer. This data preparation step can be performed using the Tokenizer API provided with Keras. We add padding to make all the vectors of same length (max_length). Below code converts the text to integer indexes, now ready to be used in Keras embedding layer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rYuEbcYc4zeX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Function to get the length of longest review\n",
        "def max_len(x):\n",
        "  return len(x.split())"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1wUsoR0J4Elx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# get the max length of review column\n",
        "dataset['Max Length'] = dataset['review'].apply(max_len)"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fUyehsFo48sQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "d69ef821-0b81-48c8-f890-b02191b77e53"
      },
      "source": [
        "dataset['Max Length'].max()"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2441"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AahyjAWJozjZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.python.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.python.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "tokenizer = Tokenizer(lower=False) # adding a OOV (Out of Vocabulary tag to identify words that are outside of corpus)\n",
        "total_reviews = np.concatenate((X_train, X_test))\n",
        "tokenizer.fit_on_texts(total_reviews)\n",
        "\n",
        "max_length = dataset['Max Length'].max()\n",
        "\n",
        "# define vocabulary size\n",
        "vocabulary_size = len(tokenizer.word_index) + 1"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zylYZYwCozjc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train_tokens = tokenizer.texts_to_sequences(X_train)\n",
        "X_test_tokens = tokenizer.texts_to_sequences(X_test)"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6TQ0u5qfozjf",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "98a1deec-e5a2-45b8-ccf0-eee96185a5f4"
      },
      "source": [
        "# Checking the length of review before and after converting to sequences\n",
        "len(X_train_tokens[0]) == len(X_train[0].split())"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AjsuMPwdRJJ6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# for x,y in zip(X_train[0].split(), X_train_tokens[0]):\n",
        "#   print(x,y)"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bBrKyYf_ozjj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "# Padding the sequences \n",
        "X_train_pad = pad_sequences(X_train_tokens, maxlen=max_length, padding='post')\n",
        "X_test_pad = pad_sequences(X_test_tokens, maxlen=max_length, padding='post')"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H87nzmHHozjl",
        "colab_type": "text"
      },
      "source": [
        "## Build the model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pWqnFkBbZGMp",
        "colab_type": "text"
      },
      "source": [
        "### **Gated Recurrent Unit**\n",
        "\n",
        "| ![GRU.jpg](https://miro.medium.com/max/1050/1*jhi5uOm9PvZfmxvfaCektw.png) | \n",
        "|:--:| \n",
        "| Figure 1: GRU Cell Picture [[3](https://towardsdatascience.com/illustrated-guide-to-lstms-and-gru-s-a-step-by-step-explanation-44e9eb85bf21)] |\n",
        "\n",
        "[This](https://towardsdatascience.com/illustrated-guide-to-lstms-and-gru-s-a-step-by-step-explanation-44e9eb85bf21) article is a great resource for <i> GRU vs LSTM</i>."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Feu1xAZVozjm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, LSTM, GRU, Dense\n",
        "# from keras.layers.embeddings import Embedding\n",
        "\n",
        "## GRU (Gated Recurrent Unit) model\n",
        "# EMBEDDING_DIM=100\n",
        "# print('Build Model..')\n",
        "\n",
        "# model = Sequential()\n",
        "# model.add(Embedding(vocabulary_size, EMBEDDING_DIM, input_length=max_length))\n",
        "# model.add(GRU(units=32, dropout=0.2, recurrent_dropout=0.2))\n",
        "# model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "# # Since this is binary classification model, lets use binary_classification as loss\n",
        "# model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0lh8TiPDXPbJ",
        "colab_type": "text"
      },
      "source": [
        "### **Global Average Pooling** \n",
        "Global average and global max pooling reduce the spatial size of the feature map/representation to one feature map for each category (classification task).\n",
        "\n",
        "**Example:** Let's say we have an image of a cat. Global average pooling will take the average of all the actiavtion of all the activation values and tell us about the overall strength of the image, i.e. whether the image is of the cat or not. \n",
        "\n",
        "Global max pooling, on the other hand will take the maximum of the activation values. This will help identify the strongest trait of the image, say, ears of the dog. Similarity, in the case of textual data, global max pooling highlights the phrase with the most information while global average pooling indicates the overall value of the sentence. [[4](https://www.datasciencediscovery.com/index.php/2019/03/11/nlp-with-dl/)]"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tOp3kxbkozjp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "EMBEDDING_DIM=100\n",
        "\n",
        "# Global Average Pooling\n",
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Embedding(vocabulary_size, EMBEDDING_DIM, input_length=max_length),\n",
        "    tf.keras.layers.GlobalAveragePooling1D(),\n",
        "    tf.keras.layers.Dense(24, activation='relu'),\n",
        "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
        "])\n",
        "model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XccZ-4iJbUQD",
        "colab_type": "text"
      },
      "source": [
        "### **LSTM (Long Short Term Memory)**\n",
        "\n",
        "LSTM has been an improvement over vanilla RNN as they are able to capture long term dependencies by introducing input, forget and output gates, which control what previous information needs to be stored and updated. [[1](https://www.datasciencediscovery.com/index.php/2019/03/11/nlp-with-dl/)]\n",
        "\n",
        "LSTM is based on the following 3 major ideas:\n",
        "* Introduing a word to allow my neural network to learn from it\n",
        "* Does a previously learned word continue to make sense or should I forget about it?\n",
        "* Creating the final Output that is getting the predicted value."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0dbfN136zJ00",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## LSTM Model \n",
        "# EMBEDDING_DIM=128\n",
        "\n",
        "# model = tf.keras.Sequential([\n",
        "#     tf.keras.layers.Embedding(vocabulary_size, EMBEDDING_DIM, input_length=max_length),\n",
        "#     tf.keras.layers.LSTM(units=60, activation='tanh'),\n",
        "#     tf.keras.layers.Dense(1, activation='sigmoid')\n",
        "# ])\n",
        "# model.compile(loss='binary_crossentropy', optimizer='adam', metrics=[\"accuracy\"])"
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dhZ3GfDbozjr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "outputId": "f727f022-a4cd-4411-d479-c47828a51ed9"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_1 (Embedding)      (None, 2441, 100)         21572200  \n",
            "_________________________________________________________________\n",
            "global_average_pooling1d (Gl (None, 100)               0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 24)                2424      \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 1)                 25        \n",
            "=================================================================\n",
            "Total params: 21,574,649\n",
            "Trainable params: 21,574,649\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LX70LKFUozjs",
        "colab_type": "text"
      },
      "source": [
        "## Train the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nP5cHr99ozjt",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 936
        },
        "outputId": "0772aac7-a281-4dc7-8d25-2895368e4e3e"
      },
      "source": [
        "%%time\n",
        "print('Train..')\n",
        "\n",
        "model.fit(X_train_pad, y_train, batch_size=128, epochs=25, validation_data=(X_test_pad, y_test), verbose=2)"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train..\n",
            "Epoch 1/25\n",
            "196/196 - 37s - loss: 0.6932 - accuracy: 0.4968 - val_loss: 0.6931 - val_accuracy: 0.5010\n",
            "Epoch 2/25\n",
            "196/196 - 37s - loss: 0.6914 - accuracy: 0.5349 - val_loss: 0.6844 - val_accuracy: 0.5459\n",
            "Epoch 3/25\n",
            "196/196 - 37s - loss: 0.6456 - accuracy: 0.6599 - val_loss: 0.5886 - val_accuracy: 0.7033\n",
            "Epoch 4/25\n",
            "196/196 - 37s - loss: 0.4885 - accuracy: 0.8188 - val_loss: 0.4153 - val_accuracy: 0.8531\n",
            "Epoch 5/25\n",
            "196/196 - 37s - loss: 0.3526 - accuracy: 0.8740 - val_loss: 0.3480 - val_accuracy: 0.8665\n",
            "Epoch 6/25\n",
            "196/196 - 37s - loss: 0.2846 - accuracy: 0.9008 - val_loss: 0.3136 - val_accuracy: 0.8784\n",
            "Epoch 7/25\n",
            "196/196 - 37s - loss: 0.2473 - accuracy: 0.9106 - val_loss: 0.2981 - val_accuracy: 0.8832\n",
            "Epoch 8/25\n",
            "196/196 - 38s - loss: 0.2136 - accuracy: 0.9246 - val_loss: 0.2852 - val_accuracy: 0.8880\n",
            "Epoch 9/25\n",
            "196/196 - 37s - loss: 0.1931 - accuracy: 0.9325 - val_loss: 0.2796 - val_accuracy: 0.8918\n",
            "Epoch 10/25\n",
            "196/196 - 37s - loss: 0.1703 - accuracy: 0.9437 - val_loss: 0.2773 - val_accuracy: 0.8909\n",
            "Epoch 11/25\n",
            "196/196 - 37s - loss: 0.1495 - accuracy: 0.9515 - val_loss: 0.2720 - val_accuracy: 0.8964\n",
            "Epoch 12/25\n",
            "196/196 - 37s - loss: 0.1352 - accuracy: 0.9578 - val_loss: 0.2713 - val_accuracy: 0.8966\n",
            "Epoch 13/25\n",
            "196/196 - 37s - loss: 0.1207 - accuracy: 0.9630 - val_loss: 0.2794 - val_accuracy: 0.8936\n",
            "Epoch 14/25\n",
            "196/196 - 37s - loss: 0.1090 - accuracy: 0.9672 - val_loss: 0.2734 - val_accuracy: 0.8966\n",
            "Epoch 15/25\n",
            "196/196 - 37s - loss: 0.0990 - accuracy: 0.9710 - val_loss: 0.2758 - val_accuracy: 0.8970\n",
            "Epoch 16/25\n",
            "196/196 - 40s - loss: 0.0896 - accuracy: 0.9741 - val_loss: 0.2985 - val_accuracy: 0.8899\n",
            "Epoch 17/25\n",
            "196/196 - 38s - loss: 0.0798 - accuracy: 0.9773 - val_loss: 0.2836 - val_accuracy: 0.8962\n",
            "Epoch 18/25\n",
            "196/196 - 38s - loss: 0.0748 - accuracy: 0.9780 - val_loss: 0.3075 - val_accuracy: 0.8901\n",
            "Epoch 19/25\n",
            "196/196 - 38s - loss: 0.0646 - accuracy: 0.9836 - val_loss: 0.3072 - val_accuracy: 0.8915\n",
            "Epoch 20/25\n",
            "196/196 - 37s - loss: 0.0575 - accuracy: 0.9852 - val_loss: 0.3014 - val_accuracy: 0.8947\n",
            "Epoch 21/25\n",
            "196/196 - 36s - loss: 0.0522 - accuracy: 0.9876 - val_loss: 0.3268 - val_accuracy: 0.8896\n",
            "Epoch 22/25\n",
            "196/196 - 37s - loss: 0.0472 - accuracy: 0.9886 - val_loss: 0.3141 - val_accuracy: 0.8937\n",
            "Epoch 23/25\n",
            "196/196 - 37s - loss: 0.0432 - accuracy: 0.9902 - val_loss: 0.3327 - val_accuracy: 0.8908\n",
            "Epoch 24/25\n",
            "196/196 - 36s - loss: 0.0375 - accuracy: 0.9914 - val_loss: 0.3259 - val_accuracy: 0.8940\n",
            "Epoch 25/25\n",
            "196/196 - 37s - loss: 0.0339 - accuracy: 0.9930 - val_loss: 0.3425 - val_accuracy: 0.8914\n",
            "CPU times: user 27min 20s, sys: 1min 4s, total: 28min 24s\n",
            "Wall time: 15min 33s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R_VyQtZlwDKR",
        "colab_type": "text"
      },
      "source": [
        "### Test the Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yuWT3Op6ozjv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "560dfe2c-c59f-4cda-ff96-b1194c55ba92"
      },
      "source": [
        "test_1 = \"This movie was awesome!! I really liked it..\"\n",
        "test_2 = \"It was really bad movie, I did not like it.\"\n",
        "test = [test_1, test_2]\n",
        "test = clean_data(test)\n",
        "\n",
        "test_sample_token = tokenizer.texts_to_sequences(test)\n",
        "test_sample_pad_seq = pad_sequences(test_sample_token, maxlen=max_length)"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 2/2 [00:00<00:00, 538.28it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q-pTSRPvozjx",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "f5126492-9766-48ee-f6c9-676c7c6d37b5"
      },
      "source": [
        "# Predict the output\n",
        "model.predict(x=test_sample_pad_seq)"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.8603076 ],\n",
              "       [0.37597716]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CN7fsgAgyykC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Save the model to Json file\n",
        "model_json = model.to_json() # De-searialize the model so that we can save in json file\n",
        "with open('/gdrive/My Drive/data/IMDB_model.json', 'w') as fp:\n",
        "  fp.write(model_json)"
      ],
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "glUOY1Xu0IWz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Or you can also save the model weights to HDF5\n",
        "model.save_weights('/gdrive/My Drive/data/Imdb_model_HDF5.h5')"
      ],
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VCvdxnbgWW6N",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
